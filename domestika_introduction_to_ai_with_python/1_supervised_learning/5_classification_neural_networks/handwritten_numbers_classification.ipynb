{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use tensorflow's `mnist` dataset, which allows us to classify handwritten numbers.\n",
    "\n",
    "The `mnist` dataset has been separated into:\n",
    "- 60,000 samples for training\n",
    "- 10,000 samples for testing\n",
    "\n",
    "## Feature\n",
    "\n",
    "Each data has two features: `image`, `label`\n",
    "- `image` has the class of `Image`, with the shape of (`x_pixel`, `y_pixel`, `color_channel`), e.g., (28, 28, 1), which means 28 by 28 pixel with the color channel of 1 meaning black and white.\n",
    "  - The `color_channel` will be 3 if it is colored (3 stands for Red, Green, and Blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: Use tfds.load('dataset_name') to load datasets from TensorFlow\n",
    "data, metadata = tfds.load('mnist', as_supervised=True, with_info=True)\n",
    "# TONOTE: `as_supervised` tells TensorFlow to load the dataset in a supervised format, meaning each data point is returned as a tuple (input, label). For example, (image, label).\n",
    "# TONOTE: `with_info` includes metadata about the dataset, such as its description, version, and features. It’s like getting a user manual along with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the train and test data\n",
    "\n",
    "data_train = data['train']\n",
    "data_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "metadata.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: `metadata.features['label']` is a ClassLabel, according to the tensorflow docs, it has an attribute called `.names` which returns the string names of the classes. Since the `num_classes=10`, the string name defaults to ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "class_names: list = metadata.features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: Each pixel ranges from 0 to 255 (which is represented by 1 byte), it's a good idea to normalize the data before training, because all models work much between if the input values are scaled to smaller numbers (from 0 to 1).\n",
    "\n",
    "# TONOTE: We will do this for both the training and testing data. To not repeat ourselves, first we will create a function called normalizer, then using python's map function to map each data to the normalizer function\n",
    "\n",
    "def normalizer(images, labels):\n",
    "    # TONOTE: Since the image data are integers from 0 to 255, normalizing it will make it float instead, so we'd better convert the numbers to float32 first\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    # We want to convert the numbers from 0.0 to 255.0 to 0.0 and 1.0, we will divide them by 255\n",
    "    images = images / 255\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "data_train = data_train.map(normalizer)\n",
    "data_test = data_test.map(normalizer)\n",
    "\n",
    "# TONOTE: Save data to cache to process faster from the second time on\n",
    "data_train = data_train.cache()\n",
    "data_test = data_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a new dataset that contains the first element from `data_train` using tensorflow's take function\n",
    "first_element = data_train.take(1)\n",
    "\n",
    "# Iterate over the first element to inspect it\n",
    "for images, labels in first_element:\n",
    "    # print(images, labels)\n",
    "    break\n",
    "# TONOTE: The `for` loop seems to not do anything, but with it, the variables `images` and `labels` have been set and ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Plotting a sample image\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Replot with to a grayscale image\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images, cmap=plt.cm.binary) # TONOTE: The images actually only has one channel of color (1), that is the grayscale color, we still need to specify the cmap value here though. If we don't, the system will randomly adds colors to the plot instead of the grayscale.\n",
    "plt.colorbar() # Here we see that the value is between 0 to 1, instead of 0 to 255\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Let's show the first 25 images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, (images, labels) in enumerate(data_train.take(25)): # TONOTE: We use `.take(n)` to get the fist `n` elements from data_train\n",
    "    # TONOTE: Use subplot of matplotlib to return multi-plots by specifying the number of row, column, and index\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(images, cmap=plt.cm.binary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model to train our data\n",
    "\n",
    "Our data is of the shape (28, 28, 1), which is a 3D tensor. We will flatten this to a 1D tensor of shape (784,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TONOTE: Flattening data which will be used as the input feature in the first layer\n",
    "flatten_data = keras.layers.Flatten(input_shape=(28,28,1))\n",
    "# TONOTE: The `keras.layers.Flatten` method will convert the (28, 28,1) input data into the shape of (28*28,) or (784,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # TONOTE: This flatten data is the first layer, which acts like the Input Layer in other models. However, since the name `Input Layer` logically ties with `keras.layers.Input(...)` and the used of `keras.layers.Input(...)` in the sequential model should be avoided, it's best to not call it `Input Layer` here.\n",
    "    flatten_data, # Flatten layer (first layer)\n",
    "    # Hidden layer\n",
    "    keras.layers.Dense(1),\n",
    "    # Output layer\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax) # Because there are 10 possible answers, namely 0, 1, 2, ..., 9\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# # TONOTE:\n",
    "# keras.Sequential([\n",
    "#     keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "#     keras.layers.Dense(1),\n",
    "#     keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])\n",
    "\n",
    "# # is the same as:\n",
    "\n",
    "# keras.Sequential([\n",
    "#     keras.layers.Dense(1, input_shape=(784,)),\n",
    "#     keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: \n",
    "# keras.losses.SparseCategoricalCrossentropy() is a loss function, specifically designed for multi-class classification problems.\n",
    "# # - It's a variant of the categorical cross-entropy loss function, but it's used when the labels are integers instead of one-hot encoded vectors. This means that instead of having a vector of probabilities for each class, we have a single integer representing the class label.\n",
    "# # - SparseCategoricalCrossentropy is a good choice when:\n",
    "# # # - We have a multi-class classification problem (e.g., image classification with multiple classes)\n",
    "# # # - Our labels are integers (not one-hot encoded)\n",
    "# # # - It's not specific to image classification, but it can be used for image classification problems. In fact, it's a popular choice for image classification tasks, especially when using convolutional neural networks (CNNs).\n",
    "# However, if we're working with a binary classification problem (e.g., image classification with only two classes), we might want to use keras.losses.BinaryCrossentropy() instead.\n",
    "# In general, the choice of loss function depends on the specific problem we're trying to solve, so it's always a good idea to consider the characteristics of our problem and choose the most suitable loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: We can also choose our metrics while compiling the model: `metrics=['accuracy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Check the length of data_train and data_test\n",
    "print(f\"Training Data Length: {len(data_train)}\")\n",
    "print(f\"Testing Data Length: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TONOTE: With so many training data, we should do some pre-processing to optimize the model's performance, making it run faster and more efficiently. Create a batch size to train the model in a batch of `n` every time, instead of running one by one.:\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "data_train = data_train.repeat().shuffle(60000).batch(batch_size)\n",
    "# TONOTE: TensorFlow data is a stream. By default, it does not repeat or cycle like the way some finite and static data (like Numpy array) does. We must explicitly use `.repeat()` to make the dataset cycle indefinitely for multiple epochs.\n",
    "\n",
    "# TONOTE: The shuffle value should be equal to the entire dataset size (number of dataset).\n",
    "\n",
    "# TONOTE: The `.batch()` method allows us to run the model in batch, instead of running one by one.\n",
    "\n",
    "data_test = data_test.batch(batch_size)  # TONOTE: Since data_test only has 10000 records, it's not necessary to use the `.repeat()` and `.shuffle()` methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Show the first image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images, cmap=plt.cm.binary) # the `images` variable is actually the one set in the for loop in a previous cell\n",
    "plt.colorbar() # we see that it goes from 0 to 1, instead of 0 to 255\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    data_train, \n",
    "    epochs=10,\n",
    "    steps_per_epoch=math.ceil(60000/batch_size)\n",
    ")\n",
    "# TONOTE: Since we train the model in batch, we need to tell it how many batches to expect by specifying the `steps_per_epoch` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Show the first image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(images, cmap=plt.cm.binary) # the `images` variable is actually the one set in the for loop in the immediate upper cell\n",
    "plt.colorbar() # we see that it goes from 0 to 1, instead of 0 to 255\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that, with only one simple hidden layer, the accuracy of 42% is very low.\n",
    "\n",
    "# Try adding more layers with more neurons and activation function in the hidden layer\n",
    "model = keras.Sequential([\n",
    "    # Flatten layer\n",
    "    flatten_data,\n",
    "    # Hidden layer\n",
    "    keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    # Output layer\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Retrain the model\n",
    "history = model.fit(\n",
    "    data_train,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=math.ceil(60000/batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a grid with multiple predictions, respectively marking correct and incorrect ones as blue and red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from matplotlib.container import BarContainer\n",
    "\n",
    "# TONOTE: Get the first batch of test data for a quick inspection and make predictions, as testing the entire dataset is too time consuming\n",
    "for images_test, labels_test in data_test.take(1):\n",
    "    # TONOTE: Convert the images_test and labels_test from TensorFlow tensors into numpy arrays so that they can be easily used with Matplotlib\n",
    "    images_test: np.ndarray = images_test.numpy()\n",
    "    labels_test: np.ndarray = labels_test.numpy()\n",
    "    # TONOTE: `images_test.shape` returns (32, 28, 28, 1), 32 is the batch_size, 28 is the width, 28 is the height, 1 is the color channel (grayscale). The shape already matches the one expected by `model.predict`, so we don't need to reshape it.\n",
    "    predictions: np.ndarray = model.predict(images_test)\n",
    "    \n",
    "# Function to plot individual images with their predictions\n",
    "def plot_image(i: int, predictions: np.ndarray, true_labels: np.ndarray, images: np.ndarray) -> None:\n",
    "    # Extract data for the specific index\n",
    "    prediction, true_label, image = predictions[i], true_labels[i], images[i]\n",
    "    prediction: np.ndarray\n",
    "    true_label: np.int64\n",
    "    image: np.ndarray\n",
    "    \n",
    "    # Remove grid and axis ticks for cleaner visualization\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Display the image (only first channel as it's grayscale)\n",
    "    plt.imshow(image[...,0], cmap=plt.cm.binary)\n",
    "    # TONOTE: each image has the shape of (28, 28, 1), the ellipsis literal image[...,0] converts its shape to (28, 28). This is not necessary to convert the shape of (28, 28, 1) to (28, 28) though, as in this case, matplotlib can plot the exact same graph using both shapes. Here we simply show that it works with both (28, 28) and (28, 28, 1).\n",
    "    \n",
    "    # TONOTE: Next, we need to get the predicted label. `prediction` is a 1D numpy array with the shape of (10,). The array may look like this: `[4.5158291e-12 2.1048035e-12 9.9999988e-01 8.6977693e-08 8.7193273e-12 2.4434133e-12 1.8026095e-11 1.8544542e-10 5.9155116e-08 7.4279107e-12]`, where each item is the predicted probability for each digit class (0 through 9). Therefore, to get the predicted label, we use `np.argmax` to return the \"index\" of the maximum value in the array\n",
    "    predicted_label: np.int64 = np.argmax(prediction)\n",
    "    \n",
    "    # Set color for the predicted label: blue for correct predictions, red for incorrect\n",
    "    color = 'blue' if predicted_label == true_label else 'red'\n",
    "    \n",
    "    print(f\"Metadata: {metadata}\")\n",
    "    print(f\"Metadata label: {metadata.features['label']}\")\n",
    "    print(f\"Metadata label name: {metadata.features['label'].names}\")\n",
    "    \n",
    "    # Add label showing: predicted digit, confidence percentage, and true digit\n",
    "    ## TONOTE: This shows the predicted label, probability or percentage of the prediction, and the true label. By showing these values, the model's prediction can be easily evaluated.\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(\n",
    "        class_names[predicted_label],\n",
    "        100*np.max(prediction),\n",
    "        class_names[true_label]\n",
    "    ), color=color)\n",
    "    # TONOTE: The three placeholders, `{}`, `{:2.0f}`, and `({})`, match their corresponding values, namely `class_names[predicted_label]`, 100*np.max(prediction)`, and `class_names[true_label]`\n",
    "\n",
    "# Function to plot probability distribution for each prediction\n",
    "def plot_value_array(i: int, predictions: np.ndarray, true_labels: np.ndarray) -> None:\n",
    "    prediction, true_label = predictions[i], true_labels[i]\n",
    "    prediction: np.ndarray\n",
    "    true_label: np.int64\n",
    "    \n",
    "    # Remove grid and axis ticks\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Create bar chart of probability distribution (0-9)\n",
    "    graph: BarContainer = plt.bar(range(10), prediction, color=\"#777777\")\n",
    "    plt.ylim([0,1])  # Set y-axis limit to 0-1 for probabilities\n",
    "    ## TONOTE: Matplotlib's default behavior is to add a 10% margin to the y-axis limits to make the plot look more visually appealing. Setting `ylim([0,1])` is to ensure the y axis only range between 0 and 1, no more and no less.\n",
    "    \n",
    "    predicted_label: np.int64 = np.argmax(prediction)\n",
    "    \n",
    "    # Highlight bars: red for prediction, blue for true label\n",
    "    ## TONOTE: Since the labels range from 0 to 9, this is the same as x axis values. To color a bar in this case, we can use the `.set_color('some_color')` method of the bar graph on the specific bar identified by graph[index], like graph[predicted_label] or graph[true_label]\n",
    "    graph[predicted_label].set_color('red')\n",
    "    graph[true_label].set_color('blue')\n",
    "    \n",
    "# Set up grid dimensions for visualization\n",
    "num_row = 5\n",
    "num_column = 5\n",
    "num_images = num_row * num_column\n",
    "\n",
    "# Create figure with subplots for both images and their probability distributions\n",
    "plt.figure(figsize=(2*2*num_column, 2*num_row))\n",
    "for i in range(num_images):\n",
    "    # TONOTE: Create subplot for image (2*i+1 for left column) by specifying the number of rows, columns, and index (in this case: 1, 3, 5, ...)\n",
    "    plt.subplot(num_row, 2*num_column, 2*i+1)\n",
    "    plot_image(i, predictions, labels_test, images_test)\n",
    "    # TONOTE: Create subplot for probability distribution (2*i+2 for right column); The index in this case is 2, 4, 6, ...\n",
    "    plt.subplot(num_row, 2*num_column, 2*i+2)\n",
    "    plot_value_array(i, predictions, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Use of Ellipsis and ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][...,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][...,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "images_test[0][...,0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 4D array\n",
    "array_4d = np.random.rand(2, 3, 4, 5)\n",
    "print(array_4d.shape)  # Output: (2, 3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[[1_1_1_1, 1_1_1_2, 1_1_1_3],\n",
    "   [2_1_1_1, 2_1_1_2, 2_1_1_3]],\n",
    "  \n",
    "  [[1_1_2_1, 1_1_2_2, 1_1_2_3],\n",
    "   [2_1_2_1, 2_1_2_2, 2_1_2_3]]],\n",
    "\n",
    " [[[1_2_1_1, 1_2_1_2, 1_2_1_3],\n",
    "   [2_2_1_1, 2_2_1_2, 2_2_1_3]],\n",
    "\n",
    "  [[1_2_2_1, 1_2_2_2, 1_2_2_3],\n",
    "   [2_2_2_1, 2_2_2_2, 2_2_2_3]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d = np.random.rand(2, 3, 4, 5)\n",
    "array_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "array_4d_2 = np.random.rand(4, 2, 2, 3)\n",
    "array_4d_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[...,0]\n",
    "# TONOTE: By using the ellipsis convention [...,k], an array with the shape of (a_(1), a_(2), ..., a_(n-1), a_(n)) will be converted into the shape of (a_(1), a_(2), ..., a_(n-1)), where k ∈ ℕ & k < n.\n",
    "\n",
    "# If k = 0, it will take the value of the first index of each inner most column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[...,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[...,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[0,...]\n",
    "\n",
    "# TONOTE: In this case, we convert a (4, 2, 2, 3) array into the shape of (2, 2, 3). It takes only the first index of the outer most column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "array_4d_2[0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TONOTE: To take only some idex in the middle, use the `:` convention\n",
    "array_4d_2[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 2 3]\n",
      " [4 8 2 6]\n",
      " [8 2 3 0]]\n",
      "Ellipsis literal output:- [3 8 2].\n",
      "general slice notation output:- [3 8 2]\n",
      "Python Ellipsis output:- [3 8 2]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[5,3,2,3],[4,8,2,6],[8,2,3,0]])\n",
    "print(arr)\n",
    "\n",
    "# Ellipsis literal\n",
    "print(f\"Ellipsis literal output:- {arr[...,1]}.\")\n",
    "\n",
    "# general slice notation\n",
    "print(f\"general slice notation output:- {arr[:,1]}\")\n",
    "\n",
    "# Python Ellipsis \n",
    "print(f\"Python Ellipsis output:- {arr[Ellipsis, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "arr[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "arr[...,0].shape == (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "my_images = np.random.randint(0, 3, size=(3, 3))\n",
    "my_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Default Behavior of Matplotlib's color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TONOTE: No color mapping is set, but Matplotlib assigns random colors to the graph by default\n",
    "plt.imshow(my_images)\n",
    "plt.show()\n",
    "\n",
    "# We see that, even if the my_images doesn't have a color channel, the system still randomly assigns colors to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "my_images_2 = np.random.rand(3, 3)\n",
    "my_images_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.imshow(my_images, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "# TONOTE: We have to set the cmap=plt.cm.binary to get the grayscale graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a sample RGBA image\n",
    "images_rgba = np.random.randint(0, 256, size=(256, 256, 4), dtype=np.uint8)\n",
    "\n",
    "plt.imshow(images_rgba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.imshow(images_rgba, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.argmax(predictions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Learning Points Analysis\n",
    "\n",
    "## 1. Dataset Loading\n",
    "**Concept:** `tfds.load()` with specific parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = tfds.load('mnist', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `as_supervised`: Returns data as (input, label) tuples\n",
    "- `with_info`: Includes dataset metadata\n",
    "**Importance:** Proper dataset structuring for supervised learning tasks\n",
    "**Best Practice:** Always load supervised datasets with labels properly paired\n",
    "\n",
    "## 2. Label Structure\n",
    "**Concept:** `metadata.features['label']` provides ClassLabel information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = metadata.features['label'].names  # Returns ['0', '1', '2', ..., '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance:** Understanding label encoding for classification tasks\n",
    "**Best Practice:** Always check label structure before training\n",
    "\n",
    "## 3. Data Normalization\n",
    "**Concept:** Converting pixel values from 0-255 to 0-1 range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = images / 255\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance:** Models perform better with normalized input\n",
    "**Best Practice:** Always normalize before training\n",
    "**Pitfall:** Forgetting to convert to float32 first\n",
    "\n",
    "## 4. Data Caching\n",
    "**Concept:** Using TensorFlow's cache mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.cache()\n",
    "data_test = data_test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance:** Improves performance for subsequent epochs\n",
    "**Best Practice:** Cache after preprocessing but before training\n",
    "\n",
    "## 5. Model Structure Understanding\n",
    "**Concept:** Sequential model layer organization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Flatten layer acts as first layer but isn't technically \"Input Layer\"\n",
    "**Best Practice:** Avoid explicit Input layers in Sequential models\n",
    "\n",
    "## 6. Loss Function Selection\n",
    "**Concept:** SparseCategoricalCrossentropy usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Cases:**\n",
    "- Multi-class classification\n",
    "- Integer labels (not one-hot encoded)\n",
    "**Alternative:** Use BinaryCrossentropy for binary classification\n",
    "\n",
    "## 7. Data Streaming and Batching\n",
    "**Concept:** TensorFlow data streaming behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.repeat().shuffle(60000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.repeat()`: Cycles dataset indefinitely\n",
    "- `.shuffle()`: Randomizes data order\n",
    "- `.batch()`: Groups data into batches\n",
    "**Best Practice:** Set shuffle buffer size to dataset size\n",
    "\n",
    "## 8. Dimension Handling\n",
    "**Concept:** Using ellipsis notation for array manipulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[...,0]  # Converts (28, 28, 1) to (28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage:** Simplifies array dimension handling\n",
    "**Note:** Both shapes work with matplotlib for visualization\n",
    "\n",
    "## 9. Model Prediction Understanding\n",
    "**Concept:** Prediction output structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(images_test)\n",
    "predicted_label = np.argmax(prediction)  # Gets index of highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance:** Understanding how to interpret model outputs\n",
    "**Best Practice:** Use argmax to convert probabilities to class labels\n",
    "\n",
    "Each of these points was specifically marked with `TONOTE` in the notebook and represents core concepts in machine learning implementation with TensorFlow. They cover data preprocessing, model architecture, training configuration, and results interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Process Implementation Summary\n",
    "\n",
    "## 1. Data Preparation\n",
    "### Loading MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = tfds.load('mnist', as_supervised=True, with_info=True)\n",
    "data_train = data['train']\n",
    "data_test = data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used TensorFlow Datasets to load MNIST handwritten digits\n",
    "- Split into 60,000 training and 10,000 test samples\n",
    "- Each image is 28x28 pixels in grayscale (1 channel)\n",
    "\n",
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(images, labels):\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    images = images / 255\n",
    "    return images, labels\n",
    "\n",
    "data_train = data_train.map(normalizer).cache()\n",
    "data_test = data_test.map(normalizer).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalized pixel values from [0-255] to [0-1] range\n",
    "- Implemented caching for performance optimization\n",
    "\n",
    "### Batch Processing Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_train = data_train.repeat().shuffle(60000).batch(batch_size)\n",
    "data_test = data_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configured batch size of 32 for efficient training\n",
    "- Implemented data shuffling for training set\n",
    "- Applied batching to both training and test sets\n",
    "\n",
    "## 2. Model Architecture\n",
    "\n",
    "### Initial Simple Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single hidden layer\n",
    "- Achieved approximately 42% accuracy\n",
    "\n",
    "### Improved Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Added two hidden layers with 50 neurons each\n",
    "- Used ReLU activation for hidden layers\n",
    "- Softmax activation for output classification\n",
    "\n",
    "## 3. Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used Adam optimizer\n",
    "- Implemented SparseCategoricalCrossentropy loss for multi-class classification\n",
    "- Tracked accuracy as primary metric\n",
    "\n",
    "## 4. Training Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    data_train, \n",
    "    epochs=10,\n",
    "    steps_per_epoch=math.ceil(60000/batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trained for 10 epochs\n",
    "- Calculated steps per epoch based on batch size\n",
    "- Used entire training dataset of 60,000 samples\n",
    "\n",
    "## Key Technical Decisions\n",
    "1. **Data Normalization**: Implemented to improve model convergence and performance\n",
    "2. **Batch Processing**: Chosen for memory efficiency and training stability\n",
    "3. **Model Architecture Evolution**: Moved from simple to more complex architecture for better accuracy\n",
    "4. **Activation Functions**: \n",
    "   - ReLU for hidden layers to handle non-linearity\n",
    "   - Softmax for output layer to handle multi-class classification\n",
    "\n",
    "## Parameters and Configuration\n",
    "- Input Shape: (28, 28, 1)\n",
    "- Batch Size: 32\n",
    "- Hidden Layer Neurons: 50 per layer\n",
    "- Training Epochs: 10\n",
    "- Optimizer: Adam\n",
    "- Loss Function: SparseCategoricalCrossentropy\n",
    "\n",
    "The implementation follows standard practices for image classification using neural networks, with a focus on proper data preprocessing and incremental model improvement. The project demonstrates the impact of model architecture on performance, showing significant improvement when moving from a simple to a more complex network structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
