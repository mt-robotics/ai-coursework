{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI with Python - Key Learning Summary\n",
    "\n",
    "This notebook provides a comprehensive summary of the key learning points from my AI course. It's organized chronologically to follow the learning journey from data analysis to advanced AI techniques, with detailed technical information about machine learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Data Fundamentals\n",
    "1. [Data Analysis and Preparation](#data-analysis)\n",
    "2. [Data Normalization Techniques](#data-normalization)\n",
    "\n",
    "### Machine Learning Fundamentals\n",
    "3. [Linear Regression](#linear-regression)\n",
    "4. [Logistic Regression](#logistic-regression)\n",
    "5. [Decision Trees](#decision-trees)\n",
    "6. [Support Vector Machines](#svm)\n",
    "7. [K-Means Clustering](#kmeans)\n",
    "\n",
    "### Neural Networks\n",
    "8. [Neural Network Fundamentals](#nn-fundamentals)\n",
    "9. [Activation Functions](#activation-functions)\n",
    "10. [Loss Functions](#loss-functions)\n",
    "11. [Neural Networks for Regression](#nn-regression)\n",
    "12. [Neural Networks for Classification](#nn-classification)\n",
    "13. [Neural Network Layers](#nn-layers)\n",
    "\n",
    "### Advanced Techniques\n",
    "14. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "15. [Model Optimization Strategies](#model-optimization)\n",
    "16. [Data Augmentation](#data-augmentation)\n",
    "17. [Transfer Learning](#transfer-learning)\n",
    "18. [Model Deployment](#model-deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib scikit-learn tensorflow keras seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries used throughout this summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set plot styling\n",
    "plt.style.use('ggplot')\n",
    "sns.set()\n",
    "\n",
    "# For reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-analysis'></a>\n",
    "## 1. Data Analysis and Preparation\n",
    "\n",
    "### Key Learning Points:\n",
    "- **Data visualization** is crucial for understanding your dataset\n",
    "- **Handling missing values** is a critical preprocessing step\n",
    "- **Categorical data** must be encoded for machine learning algorithms\n",
    "- **Feature correlation** helps identify important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and explore a sample dataset\n",
    "# Replace with your own dataset or use a built-in one\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load sample data\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['PRICE'] = housing.target\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distributions\n",
    "df.hist(figsize=(15, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Common strategies for dealing with missing values include:\n",
    "1. **Removing rows** with missing values (good when few instances have NaNs)\n",
    "2. **Filling with statistics** like mean, median, or mode\n",
    "3. **Predicting missing values** using other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a sample dataframe with missing values\n",
    "sample_df = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': [1, 2, 3, 4, np.nan]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame with NaN values:\")\n",
    "print(sample_df)\n",
    "\n",
    "# Strategy 1: Drop rows with any NaN\n",
    "print(\"\\nAfter dropping rows with NaN:\")\n",
    "print(sample_df.dropna())\n",
    "\n",
    "# Strategy 2: Fill NaN with mean\n",
    "print(\"\\nAfter filling NaN with column means:\")\n",
    "print(sample_df.fillna(sample_df.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Data\n",
    "\n",
    "Most machine learning algorithms require numerical input. Two common methods to convert categorical data are:\n",
    "1. **Label Encoding** - Assign a number to each category \n",
    "2. **One-Hot Encoding** - Create binary columns for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling Categorical Data\n",
    "\n",
    "Most machine learning algorithms require numerical input. Two common methods to convert categorical data are:\n",
    "1. **Label Encoding** - Assign a number to each category \n",
    "2. **One-Hot Encoding** - Create binary columns for each category\n",
    "3. **Target Encoding** - Replace categories with the mean target value for that category\n",
    "4. **Binary Encoding** - Convert the category to binary code, then use the digits as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-normalization'></a>\n",
    "## 2. Data Normalization Techniques\n",
    "\n",
    "### Key Learning Points:\n",
    "- Data normalization is crucial for many machine learning algorithms\n",
    "- It ensures all features contribute equally to model predictions\n",
    "- Different techniques have different use cases and properties\n",
    "- Normalization speeds up training and improves model convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Different normalization techniques\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample data with outliers\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(0, 1, 1000)\n",
    "# Add outliers\n",
    "data = np.append(data, [10, -10, 8, -9])\n",
    "data = data.reshape(-1, 1)  # Reshape for sklearn\n",
    "\n",
    "# Apply different scalers\n",
    "scalers = {\n",
    "    'Raw Data': None,\n",
    "    'Min-Max Scaling': MinMaxScaler(),\n",
    "    'Z-Score Normalization': StandardScaler(),\n",
    "    'Robust Scaling': RobustScaler(),\n",
    "    'L2 Normalization': Normalizer()\n",
    "}\n",
    "\n",
    "# Create figure for visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot each scaling method\n",
    "for i, (name, scaler) in enumerate(scalers.items()):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    \n",
    "    if scaler:\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        scaled_data = data\n",
    "        \n",
    "    plt.hist(scaled_data, bins=50)\n",
    "    plt.title(name)\n",
    "    plt.xlim(-3, 3) if name != 'Raw Data' else plt.xlim(-12, 12)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics before and after scaling\n",
    "print(\"Raw data statistics:\")\n",
    "print(f\"  Min: {data.min():.2f}, Max: {data.max():.2f}\")\n",
    "print(f\"  Mean: {data.mean():.2f}, Std: {data.std():.2f}\")\n",
    "\n",
    "scaled = StandardScaler().fit_transform(data)\n",
    "print(\"\\nStandardized data statistics:\")\n",
    "print(f\"  Min: {scaled.min():.2f}, Max: {scaled.max():.2f}\")\n",
    "print(f\"  Mean: {scaled.mean():.2f}, Std: {scaled.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Techniques Comparison\n",
    "\n",
    "| Technique | Formula | Use Case | Pros | Cons |\n",
    "|-----------|---------|----------|------|------|\n",
    "| **Min-Max Scaling** | $(x - min) / (max - min)$ | Neural networks, when bounded output is needed | Preserves shape, maps to fixed range [0,1] | Sensitive to outliers |\n",
    "| **Z-Score Normalization** | $(x - mean) / std$ | SVM, Linear/Logistic Regression | Handles outliers better than Min-Max | Output not bounded, may exceed desired range |\n",
    "| **Robust Scaling** | $(x - median) / IQR$ | When outliers are present | Very robust to outliers | Not as widely used as Z-score |\n",
    "| **L2 Normalization** | $x / \\sqrt{\\sum{x^2}}$ | Text processing, when direction matters | Preserves direction | Changes magnitude completely |\n",
    "\n",
    "Choosing the right scaling technique depends on:\n",
    "1. The nature of your data (outliers, distribution)\n",
    "2. The algorithm you're using (some require specific scaling)\n",
    "3. The importance of interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create sample height-weight data\n",
    "np.random.seed(42)\n",
    "heights = np.random.normal(170, 10, 100)  # Mean 170cm, std 10cm\n",
    "weights = heights * 0.6 + np.random.normal(0, 5, 100)  # Weight = height*0.6 + noise\n",
    "\n",
    "# Reshape for scikit-learn (should be 2D)\n",
    "X = heights.reshape(-1, 1)  # Independent variable (height)\n",
    "y = weights  # Dependent variable (weight)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model coefficient (slope): {model.coef_[0]:.4f}\")\n",
    "print(f\"Model intercept: {model.intercept_:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the linear regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual data')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Linear model')\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.title('Height vs Weight: Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multiple Linear Regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use California housing dataset\n",
    "X = df.drop('PRICE', axis=1)\n",
    "y = df['PRICE']\n",
    "\n",
    "# Scale the features (important for multiple regression)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "multi_model = LinearRegression()\n",
    "multi_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = multi_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Display feature importance (coefficients)\n",
    "coef_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': multi_model.coef_})\n",
    "coef_df = coef_df.sort_values('Coefficient', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logistic-regression'></a>\n",
    "## 3. Logistic Regression\n",
    "\n",
    "### Key Learning Points:\n",
    "- Used for **binary classification** problems\n",
    "- Predicts probability using a **sigmoid function** (values between 0 and 1)\n",
    "- Can be extended to multi-class classification\n",
    "- Evaluation uses **accuracy, precision, recall, F1-score**\n",
    "- Key parameter: `max_iter` (increase for complex datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load breast cancer dataset (binary classification)\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "log_model = LogisticRegression(max_iter=1000)  # Increased for convergence\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_model.predict(X_test)\n",
    "y_prob = log_model.predict_proba(X_test)  # Probability estimates\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Making a prediction for a new data point\n",
    "\n",
    "# Get feature names\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Create a new hypothetical sample (using mean values)\n",
    "new_sample = X[0].reshape(1, -1)  # Using the first sample as an example\n",
    "\n",
    "# Scale the new sample using the same scaler\n",
    "new_sample_scaled = scaler.transform(new_sample)\n",
    "\n",
    "# Get prediction\n",
    "prediction = log_model.predict(new_sample_scaled)\n",
    "probability = log_model.predict_proba(new_sample_scaled)\n",
    "\n",
    "print(f\"Predicted class: {prediction[0]} ({'Benign' if prediction[0] == 1 else 'Malignant'})\")\n",
    "print(f\"Probability of malignant: {probability[0][0]:.4f}\")\n",
    "print(f\"Probability of benign: {probability[0][1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision-trees'></a>\n",
    "## 4. Decision Trees\n",
    "\n",
    "### Key Learning Points:\n",
    "- Decision trees make predictions by following a series of decisions\n",
    "- They can handle both **classification and regression** problems\n",
    "- Decision trees are easy to interpret and visualize\n",
    "- Risk of overfitting if tree is too deep\n",
    "- Hyperparameters like `max_depth` control model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Use breast cancer dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)  # Limiting depth to prevent overfitting\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_model, feature_names=cancer.feature_names, \n",
    "          class_names=['Malignant', 'Benign'], filled=True, rounded=True)\n",
    "plt.title('Decision Tree for Breast Cancer Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance in Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance from decision tree\n",
    "importances = pd.DataFrame({'Feature': cancer.feature_names, 'Importance': dt_model.feature_importances_})\n",
    "importances = importances.sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances)\n",
    "plt.title('Top 10 Feature Importance in Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nn-regression'></a>\n",
    "## 5. Neural Networks for Regression\n",
    "\n",
    "### Key Learning Points:\n",
    "- Neural networks can model complex non-linear relationships\n",
    "- Architecture design is crucial (number of layers, neurons per layer)\n",
    "- **Activation functions** add non-linearity\n",
    "- Common activations for hidden layers: ReLU, tanh, sigmoid\n",
    "- Output layer for regression: Linear activation (or none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Neural Network for Regression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a simple temperature conversion dataset (°F to °C)\n",
    "fahrenheit = np.linspace(-200, 200, 100)\n",
    "celsius = (fahrenheit - 32) * 5/9\n",
    "\n",
    "# Reshape for neural network input\n",
    "X = fahrenheit.reshape(-1, 1)  # Input: temperature in Fahrenheit\n",
    "y = celsius  # Output: temperature in Celsius\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(1,)),  # Hidden layer with 16 neurons\n",
    "    layers.Dense(8, activation='relu'),                     # Another hidden layer\n",
    "    layers.Dense(1)                                         # Output layer (no activation for regression)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # Mean squared error for regression\n",
    "    metrics=['mae']  # Mean absolute error\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0  # Set to 1 to see training progress\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Mean Absolute Error: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Mean Absolute Error over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and visualize results\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('Temperature (°F)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Neural Network: Fahrenheit to Celsius Conversion')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nn-classification'></a>\n",
    "## 6. Neural Networks for Classification\n",
    "\n",
    "### Key Learning Points:\n",
    "- Output layer uses **softmax activation** for multi-class classification\n",
    "- Loss function is typically **categorical cross-entropy**\n",
    "- Convolutional Neural Networks (CNNs) are ideal for image data\n",
    "- Data preprocessing: normalization, resizing, augmentation\n",
    "- Transfer learning leverages pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Neural Network for Classification (MNIST digits)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Data preprocessing\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Display a sample image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title(f'Label: {y_train[0]}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Reshape images for the model (flatten)\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], 28*28)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network for digit classification\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(28*28,)),\n",
    "    layers.Dropout(0.2),  # Prevent overfitting\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes (digits 0-9)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_flat, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test_flat, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def plot_prediction(i, model):\n",
    "    # Make prediction\n",
    "    prediction = model.predict(x_test_flat[i:i+1])[0]\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    true_label = y_test[i]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Display image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    plt.title(f'True: {true_label}, Predicted: {predicted_label}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display probability distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(range(10), prediction)\n",
    "    plt.xticks(range(10))\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Prediction Probabilities')\n",
    "    \n",
    "    # Highlight correct and predicted\n",
    "    bars[true_label].set_color('green')\n",
    "    if predicted_label != true_label:\n",
    "        bars[predicted_label].set_color('red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions for a few examples\n",
    "for i in range(5):\n",
    "    plot_prediction(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: CNN for MNIST\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# Reshape data for CNN (add channel dimension)\n",
    "x_train_cnn = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test_cnn = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = keras.Sequential([\n",
    "    # Convolutional layers\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Training the CNN model would require more computational resources and time. In a real setting, you would run:\n",
    "\n",
    "```python\n",
    "# Train the CNN model\n",
    "cnn_history = cnn_model.fit(\n",
    "    x_train_cnn, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test_cnn, y_test)\n",
    "print(f\"CNN Test accuracy: {cnn_test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "CNNs typically achieve higher accuracy (>99%) on MNIST compared to simple neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>\n",
    "## 7. Support Vector Machines\n",
    "\n",
    "### Key Learning Points:\n",
    "- SVMs find the optimal hyperplane that separates classes\n",
    "- **Kernel functions** transform data into higher dimensions\n",
    "- Common kernels: linear, rbf (radial basis function), polynomial\n",
    "- Key parameters: `C` (regularization) and `gamma` (kernel coefficient)\n",
    "- Effective for high-dimensional data with relatively few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Support Vector Machine\n",
    "from sklearn import svm\n",
    "\n",
    "# Use breast cancer dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train models with different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "models = {}\n",
    "scores = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Create and train the model\n",
    "    models[kernel] = svm.SVC(kernel=kernel, gamma='auto', probability=True)\n",
    "    models[kernel].fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = models[kernel].predict(X_test)\n",
    "    scores[kernel] = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"SVM with {kernel} kernel - Accuracy: {scores[kernel]:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare kernel performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(scores.keys(), scores.values())\n",
    "plt.ylim(0.9, 1.0)  # Focus on the relevant range\n",
    "plt.title('SVM Performance with Different Kernels')\n",
    "plt.xlabel('Kernel')\n",
    "plt.ylabel('Accuracy')\n",
    "for kernel, score in scores.items():\n",
    "    plt.text(kernel, score, f\"{score:.4f}\", ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: SVM Parameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# Create the grid search model\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=0, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate with best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Test accuracy with tuned model: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kmeans'></a>\n",
    "## 8. K-Means Clustering\n",
    "\n",
    "### Key Learning Points:\n",
    "- **Unsupervised learning** algorithm for finding groups in data\n",
    "- Groups data into `k` clusters based on similarity\n",
    "- The algorithm iteratively assigns points to the nearest cluster centroid\n",
    "- Determining optimal `k` can use the Elbow Method or Silhouette Score\n",
    "- Preprocessing (scaling) is important for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: K-means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data with 4 clusters\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=42)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.title('Generated Dataset with 4 Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "k_range = range(1, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(X)\n",
    "    inertia.append(model.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, 'o-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means with the optimal number of clusters\n",
    "optimal_k = 4  # From the elbow method\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title(f'K-means Clustering (k={optimal_k})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nn-fundamentals'></a>\n",
    "## 8. Neural Network Fundamentals\n",
    "\n",
    "### Key Learning Points:\n",
    "- Neural networks are inspired by the human brain's structure\n",
    "- They consist of interconnected layers of artificial neurons\n",
    "- Each connection has a **weight** and each neuron has a **bias**\n",
    "- Neural networks learn by updating weights and biases through backpropagation\n",
    "- Deep Learning refers to neural networks with many hidden layers\n",
    "- Neural networks excel at finding complex patterns in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of a simple neural network\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Rectangle, FancyArrowPatch\n",
    "\n",
    "def draw_neural_network(ax, layer_sizes, weights=None):\n",
    "    \"\"\"Draw a neural network diagram on a matplotlib axis.\"\"\"\n",
    "    v_spacing = 1\n",
    "    h_spacing = 1.5\n",
    "    \n",
    "    # Draw nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_name = \"Input\" if n == 0 else \"Hidden\" if n < len(layer_sizes)-1 else \"Output\"\n",
    "        for m in range(layer_size):\n",
    "            x = n * h_spacing\n",
    "            y = (layer_size - m - 1) * v_spacing\n",
    "            circle = Circle((x, y), 0.3, fill=False)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Add labels inside the circles\n",
    "            if n == 0:\n",
    "                ax.text(x, y, f\"$x_{m+1}$\", ha='center', va='center')\n",
    "            elif n == len(layer_sizes)-1:\n",
    "                ax.text(x, y, f\"$y_{m+1}$\", ha='center', va='center')\n",
    "            else:\n",
    "                ax.text(x, y, f\"$h_{n},{m+1}$\", ha='center', va='center')\n",
    "                \n",
    "        # Add layer labels below\n",
    "        ax.text(n * h_spacing, -1, f\"{layer_name} Layer\", ha='center')\n",
    "    \n",
    "    # Draw edges\n",
    "    if weights is None:\n",
    "        # Generate random weights for visualization\n",
    "        weights = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]))\n",
    "    \n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        for i in range(layer_size_a):\n",
    "            for j in range(layer_size_b):\n",
    "                angle = 0\n",
    "                x = n * h_spacing\n",
    "                y = (layer_size_a - i - 1) * v_spacing\n",
    "                x_end = (n + 1) * h_spacing\n",
    "                y_end = (layer_size_b - j - 1) * v_spacing\n",
    "                \n",
    "                # Set edge color based on weight value\n",
    "                if n < len(weights):\n",
    "                    color = 'red' if weights[n][i, j] < 0 else 'blue'\n",
    "                    line_width = abs(weights[n][i, j]) + 0.5\n",
    "                else:\n",
    "                    color = 'black'\n",
    "                    line_width = 1.0\n",
    "                    \n",
    "                ax.add_patch(FancyArrowPatch((x, y), (x_end, y_end), \n",
    "                                            arrowstyle='->', \n",
    "                                            color=color,\n",
    "                                            linewidth=line_width,\n",
    "                                            mutation_scale=10))\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_xlim(-0.5, len(layer_sizes) * h_spacing + 0.5)\n",
    "    ax.set_ylim(-1.5, max(layer_sizes) * v_spacing + 0.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Create example neural network\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Input, Hidden, Output layer sizes\n",
    "draw_neural_network(ax, [3, 4, 2])\n",
    "plt.title('Simple Neural Network Architecture', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# Create example for forward propagation\n",
    "print(\"Neural Network Forward Propagation\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"For each neuron in a layer:\")\n",
    "print(\"1. Multiply each input by its corresponding weight\")\n",
    "print(\"2. Sum all weighted inputs and add bias\")\n",
    "print(\"3. Apply activation function to the sum\")\n",
    "print(\"\\nMathematically:\")\n",
    "print(\"z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\")\n",
    "print(\"a = activation(z)\")\n",
    "print(\"\\nwhere:\")\n",
    "print(\"z = weighted sum + bias\")\n",
    "print(\"a = activation (output of the neuron)\")\n",
    "print(\"w = weights\")\n",
    "print(\"x = inputs\")\n",
    "print(\"b = bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases\n",
    "\n",
    "Neural networks learn by adjusting two types of parameters:\n",
    "\n",
    "1. **Weights** (w): Control the strength of the connection between neurons\n",
    "   - Positive weights amplify signals\n",
    "   - Negative weights inhibit signals\n",
    "   - Larger absolute values indicate stronger influence\n",
    "\n",
    "2. **Biases** (b): Allow the neuron to shift its activation function\n",
    "   - Act like an intercept term in linear regression\n",
    "   - Help the network learn the threshold of activation\n",
    "   - Without biases, all neurons would be inactive when all inputs are zero\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "The method neural networks use to learn from data:\n",
    "\n",
    "1. **Forward Pass**: Calculate predictions using current weights\n",
    "2. **Calculate Loss**: Measure error between predictions and actual values\n",
    "3. **Backward Pass**: Compute gradients of weights with respect to the loss\n",
    "4. **Update Weights**: Adjust weights to reduce the loss\n",
    "   - w = w - learning_rate * gradient\n",
    "\n",
    "This process repeats over many iterations (epochs) until the model converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation-functions'></a>\n",
    "## 9. Activation Functions\n",
    "\n",
    "### Key Learning Points:\n",
    "- Activation functions introduce non-linearity into neural networks\n",
    "- Without them, neural networks would behave like linear models\n",
    "- Each activation function has unique properties and use cases\n",
    "- The choice of activation function affects training dynamics and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize common activation functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create input values\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Define activation functions\n",
    "activations = {\n",
    "    'Linear': lambda x: x,\n",
    "    'ReLU': lambda x: np.maximum(0, x),\n",
    "    'Sigmoid': lambda x: 1 / (1 + np.exp(-x)),\n",
    "    'Tanh': lambda x: np.tanh(x),\n",
    "    'Leaky ReLU': lambda x: np.where(x > 0, x, x * 0.1),\n",
    "    'ELU': lambda x: np.where(x > 0, x, np.exp(x) - 1),\n",
    "    'Softmax': lambda x: np.exp(x) / np.sum(np.exp(x))  # Simplified - not really for plotting\n",
    "}\n",
    "\n",
    "# Plot activation functions\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, (name, activation) in enumerate(activations.items()):\n",
    "    if name != 'Softmax':  # Skip softmax as it's only meaningful for vectors\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        plt.plot(x, activation(x))\n",
    "        plt.title(name)\n",
    "        plt.grid(True)\n",
    "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a table explaining activation functions\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "activation_table = \"\"\"\n",
    "| Activation | Formula | Range | Use Case | Advantages | Disadvantages |\n",
    "|------------|---------|-------|----------|------------|---------------|\n",
    "| **ReLU** | max(0, x) | [0, ∞] | Hidden layers in CNNs, default for many networks | Fast computation, reduces vanishing gradient | Dead neurons problem (never activate) |\n",
    "| **Sigmoid** | 1/(1+e^(-x)) | (0, 1) | Binary classification, output layer | Smooth gradient, bounded output | Vanishing gradient problem, not zero-centered |\n",
    "| **Tanh** | (e^x - e^(-x))/(e^x + e^(-x)) | (-1, 1) | Hidden layers when zero-centered output needed | Zero-centered output, bounded | Vanishing gradient problem (less severe than sigmoid) |\n",
    "| **Leaky ReLU** | max(0.01x, x) | (-∞, ∞) | Hidden layers | Addresses dead neurons problem | Still can suffer saturation in negative region |\n",
    "| **ELU** | x if x>0 else α(e^x-1) | (-α, ∞) | Hidden layers | Smooth function, addresses dead neurons | More computationally expensive than ReLU |\n",
    "| **Softmax** | e^xi/Σe^x | (0, 1) | Multi-class classification output | Converts scores to probabilities | Only used in output layer |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(activation_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss-functions'></a>\n",
    "## 10. Loss Functions\n",
    "\n",
    "### Key Learning Points:\n",
    "- Loss functions measure how well a model performs on training data\n",
    "- They quantify the difference between predictions and actual values\n",
    "- Different problems require different loss functions\n",
    "- Optimization algorithms work to minimize the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different loss functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import (\n",
    "    MeanSquaredError, \n",
    "    MeanAbsoluteError, \n",
    "    BinaryCrossentropy,\n",
    "    CategoricalCrossentropy,\n",
    "    Huber\n",
    ")\n",
    "\n",
    "# Create true and predicted values\n",
    "y_true = np.array([0, 0, 1, 1, 1])\n",
    "y_pred_range = np.linspace(0, 1, 100)\n",
    "losses = []\n",
    "\n",
    "# Calculate loss values for different prediction values\n",
    "for p in y_pred_range:\n",
    "    # Binary predictions (for binary cross-entropy)\n",
    "    y_pred_binary = np.array([p, p, p, p, p])\n",
    "    \n",
    "    # MSE\n",
    "    mse = np.mean((y_true - y_pred_binary) ** 2)\n",
    "    \n",
    "    # MAE\n",
    "    mae = np.mean(np.abs(y_true - y_pred_binary))\n",
    "    \n",
    "    # Binary Cross-Entropy\n",
    "    # Adding small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    bce = -np.mean(y_true * np.log(y_pred_binary + epsilon) + \n",
    "                  (1 - y_true) * np.log(1 - y_pred_binary + epsilon))\n",
    "    \n",
    "    # Huber Loss (delta=1.0)\n",
    "    delta = 1.0\n",
    "    error = y_true - y_pred_binary\n",
    "    huber = np.mean(np.where(np.abs(error) <= delta, \n",
    "                           0.5 * error ** 2, \n",
    "                           delta * (np.abs(error) - 0.5 * delta)))\n",
    "    \n",
    "    losses.append([mse, mae, bce, huber])\n",
    "\n",
    "losses = np.array(losses)\n",
    "\n",
    "# Plot loss functions\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "loss_names = ['Mean Squared Error', 'Mean Absolute Error', \n",
    "              'Binary Cross-Entropy', 'Huber Loss']\n",
    "\n",
    "for i, name in enumerate(loss_names):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot(y_pred_range, losses[:, i])\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Prediction (when true=0 for 2/5, true=1 for 3/5)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display loss function table\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "loss_table = \"\"\"\n",
    "| Loss Function | Formula | Use Case | Pros | Cons |\n",
    "|---------------|---------|----------|------|------|\n",
    "| **Mean Squared Error (MSE)** | $\\\\frac{1}{n}\\\\sum_{i=1}^{n}(y_i - \\\\hat{y}_i)^2$ | Regression | Penalizes large errors more | Sensitive to outliers |\n",
    "| **Mean Absolute Error (MAE)** | $\\\\frac{1}{n}\\\\sum_{i=1}^{n}\\\\lvert y_i - \\\\hat{y}_i\\\\rvert$ | Regression | More robust to outliers | Provides less gradient near optimum |\n",
    "| **Binary Cross-Entropy** | $-\\\\frac{1}{n}\\\\sum_{i=1}^{n}[y_i\\\\log(\\\\hat{y}_i) + (1-y_i)\\\\log(1-\\\\hat{y}_i)]$ | Binary classification | Ideal for probability outputs | Unstable with perfect predictions |\n",
    "| **Categorical Cross-Entropy** | $-\\\\sum_{i=1}^{n}\\\\sum_{j=1}^{m}y_{ij}\\\\log(\\\\hat{y}_{ij})$ | Multi-class classification | Works well with softmax activation | Computationally expensive |\n",
    "| **Sparse Categorical CE** | Same as CCE but with integer labels | Multi-class with integer labels | Memory efficient | Same as CCE |\n",
    "| **Huber Loss** | MSE for small errors, MAE for large errors | Regression | Combines MSE and MAE benefits | Has a hyperparameter to tune |\n",
    "| **KL Divergence** | $\\\\sum_{i=1}^{n}p(x_i)\\\\log\\\\frac{p(x_i)}{q(x_i)}$ | Comparing distributions | Measures information loss | Asymmetric |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(loss_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nn-layers'></a>\n",
    "## 13. Neural Network Layers\n",
    "\n",
    "### Key Learning Points:\n",
    "- Neural networks consist of different types of layers stacked together\n",
    "- Each layer type serves a specific purpose and has unique properties\n",
    "- Modern architectures combine multiple layer types for optimal performance\n",
    "- Layer choice depends on the data type and task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for different neural network layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a function to visualize layer transformations\n",
    "def visualize_layer_transformation(layer, input_shape, title):\n",
    "    \"\"\"Visualize how a layer transforms input data.\"\"\"\n",
    "    # Create random input data\n",
    "    if len(input_shape) == 3:  # For 2D layers (height, width, channels)\n",
    "        np.random.seed(42)\n",
    "        input_data = np.random.rand(1, *input_shape)\n",
    "        \n",
    "        # Display input\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_data[0, :, :, 0], cmap='viridis')\n",
    "        plt.title(f\"Input (shape: {input_data.shape[1:]})\")\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Apply layer and display output\n",
    "        output = layer(input_data).numpy()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if len(output.shape) == 4:  # Convolutional output\n",
    "            plt.imshow(output[0, :, :, 0], cmap='viridis')\n",
    "            plt.title(f\"Output (shape: {output.shape[1:]})\")\n",
    "        else:  # Flattened output\n",
    "            plt.bar(range(min(50, output.shape[1])), output[0, :min(50, output.shape[1])])\n",
    "            plt.title(f\"Output (shape: {output.shape[1:]})\")\n",
    "        plt.colorbar()\n",
    "        \n",
    "    else:  # For 1D layers\n",
    "        np.random.seed(42)\n",
    "        input_data = np.random.rand(1, input_shape[0])\n",
    "        \n",
    "        # Display input\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(input_shape[0]), input_data[0])\n",
    "        plt.title(f\"Input (shape: {input_data.shape[1:]})\")\n",
    "        \n",
    "        # Apply layer and display output\n",
    "        output = layer(input_data).numpy()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(range(output.shape[1]), output[0])\n",
    "        plt.title(f\"Output (shape: {output.shape[1:]})\")\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output.shape[1:]\n",
    "\n",
    "# Define various layer types to visualize\n",
    "# 1. Dense Layer (Fully Connected)\n",
    "dense_layer = layers.Dense(64, activation='relu')\n",
    "dense_output_shape = visualize_layer_transformation(dense_layer, (100,), \"Dense Layer (Fully Connected)\")\n",
    "\n",
    "# 2. Conv2D Layer (Convolutional)\n",
    "conv_layer = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')\n",
    "conv_output_shape = visualize_layer_transformation(conv_layer, (28, 28, 1), \"Conv2D Layer (Convolutional)\")\n",
    "\n",
    "# 3. MaxPooling2D Layer\n",
    "pool_layer = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "pool_output_shape = visualize_layer_transformation(pool_layer, (28, 28, 16), \"MaxPooling2D Layer\")\n",
    "\n",
    "# 4. Flatten Layer\n",
    "flatten_layer = layers.Flatten()\n",
    "flatten_output_shape = visualize_layer_transformation(flatten_layer, (14, 14, 16), \"Flatten Layer\")\n",
    "\n",
    "# 5. Dropout Layer (can't easily visualize the dropout effect)\n",
    "# Instead, create a table of layer types\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "layer_table = \"\"\"\n",
    "| Layer Type | Purpose | Use Cases | Example Parameters |\n",
    "|------------|---------|-----------|-------------------|\n",
    "| **Dense (Fully Connected)** | Basic neural network layer with connections between all neurons | General-purpose, final classification layers | `Dense(units=64, activation='relu')` |\n",
    "| **Conv2D (Convolutional)** | Applies learnable filters to detect features in images | Image classification, object detection | `Conv2D(filters=32, kernel_size=(3,3), activation='relu')` |\n",
    "| **MaxPooling2D** | Reduces spatial dimensions by taking maximum values | Downsampling feature maps, reducing parameters | `MaxPooling2D(pool_size=(2,2))` |\n",
    "| **AveragePooling2D** | Reduces spatial dimensions by averaging values | Alternative to max pooling, preserves more information | `AveragePooling2D(pool_size=(2,2))` |\n",
    "| **Flatten** | Converts multi-dimensional data to 1D | Transitioning from convolutional to dense layers | `Flatten()` |\n",
    "| **Dropout** | Randomly sets input units to 0 during training | Preventing overfitting | `Dropout(rate=0.5)` |\n",
    "| **BatchNormalization** | Normalizes layer inputs for each mini-batch | Stabilizes and accelerates training | `BatchNormalization()` |\n",
    "| **LSTM/GRU** | Processes sequential data with memory | Time series, text, speech processing | `LSTM(units=128, return_sequences=True)` |\n",
    "| **Embedding** | Maps discrete entities to vectors | Text processing, representing categorical data | `Embedding(input_dim=10000, output_dim=100)` |\n",
    "| **Add/Concatenate** | Combines outputs from multiple layers | Skip connections, ensemble models | `Concatenate()` or `Add()` |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(layer_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hyperparameter-tuning'></a>\n",
    "## 14. Hyperparameter Tuning\n",
    "\n",
    "### Key Learning Points:\n",
    "- Hyperparameters are model configuration settings set before training\n",
    "- They control model complexity, learning process, and architecture\n",
    "- Proper tuning significantly improves model performance\n",
    "- Different tuning strategies balance computational cost and effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hyperparameter tuning using grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load example dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for random forest\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit grid search (this would normally take some time)\n",
    "# We'll simulate results for faster execution\n",
    "print(\"Simulating grid search results (in practice you would run grid_search.fit(X_train, y_train))\")\n",
    "\n",
    "# Create simulated results\n",
    "np.random.seed(42)\n",
    "results = []\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            # Create a realistic but simulated accuracy score\n",
    "            base_score = 0.93\n",
    "            n_estimator_bonus = n_estimators / 1000  # More trees are better up to a point\n",
    "            depth_penalty = 0 if max_depth is None else (30 - max_depth) / 100  # Deeper can overfit\n",
    "            split_effect = (5 - abs(min_samples_split - 5)) / 100  # Best around middle value\n",
    "            random_effect = np.random.normal(0, 0.01)  # Random variation\n",
    "            \n",
    "            score = base_score + n_estimator_bonus - depth_penalty + split_effect + random_effect\n",
    "            score = min(0.99, max(0.85, score))  # Keep in realistic range\n",
    "            \n",
    "            results.append({\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth if max_depth is not None else \"None\",\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "# Convert to dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "best_params = results_df.loc[results_df['score'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    if param != 'score':\n",
    "        print(f\"  {param}: {value}\")\n",
    "print(f\"Best score: {best_params['score']:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create heatmap for each n_estimators value\n",
    "n_estimator_values = sorted(param_grid['n_estimators'])\n",
    "for i, n_est in enumerate(n_estimator_values):\n",
    "    plt.subplot(1, len(n_estimator_values), i+1)\n",
    "    \n",
    "    # Filter data for this n_estimators value\n",
    "    subset = results_df[results_df['n_estimators'] == n_est].copy()\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    heatmap_data = subset.pivot_table(\n",
    "        index='min_samples_split', \n",
    "        columns='max_depth', \n",
    "        values='score'\n",
    "    )\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title(f'n_estimators = {n_est}')\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('min_samples_split')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display hyperparameter info\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "hyperparameter_table = \"\"\"\n",
    "### Common Hyperparameters by Algorithm\n",
    "\n",
    "| Algorithm | Key Hyperparameters | Tuning Strategy |\n",
    "|-----------|---------------------|-----------------|\n",
    "| **Neural Networks** | Learning rate, batch size, epochs, number of layers, neurons per layer, dropout rate, activation functions | Start with defaults, then gradually adjust one at a time |\n",
    "| **Random Forest** | Number of trees, max depth, min samples split, max features | More trees usually better (diminishing returns), control depth to prevent overfitting |\n",
    "| **Gradient Boosting** | Learning rate, number of estimators, max depth, subsample | Small learning rate with many estimators often works well |\n",
    "| **SVM** | Kernel type, C, gamma | Start with linear kernel, then RBF; search C and gamma on log scale |\n",
    "| **k-NN** | Number of neighbors (k), distance metric, weights | Try odd values of k to avoid ties, scale features before tuning |\n",
    "\n",
    "### Tuning Approaches\n",
    "\n",
    "1. **Manual Tuning**: Adjust parameters based on experience and repeated experiments\n",
    "2. **Grid Search**: Exhaustively search through a specified parameter grid\n",
    "3. **Random Search**: Sample random combinations from parameter distributions\n",
    "4. **Bayesian Optimization**: Build a probabilistic model of the objective function\n",
    "5. **Genetic Algorithms**: Evolve parameter combinations using principles of natural selection\n",
    "6. **Automated ML (AutoML)**: Automatically search and optimize hyperparameters\n",
    "\n",
    "For neural networks, consider these additional techniques:\n",
    "- Learning rate schedulers\n",
    "- Early stopping based on validation loss\n",
    "- Cyclical learning rates\n",
    "- Weight decay (L2 regularization)\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(hyperparameter_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-augmentation'></a>\n",
    "## 16. Data Augmentation\n",
    "\n",
    "### Key Learning Points:\n",
    "- Data augmentation artificially expands the training dataset\n",
    "- It creates new training examples by applying transformations to existing data\n",
    "- Helps prevent overfitting and increases model robustness\n",
    "- Common in image, audio, and text classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Image data augmentation with Keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple 3x3 checkered image as a sample\n",
    "sample_image = np.zeros((100, 100, 3))\n",
    "for i in range(0, 100, 20):\n",
    "    for j in range(0, 100, 20):\n",
    "        if (i // 20 + j // 20) % 2 == 0:\n",
    "            sample_image[i:i+20, j:j+20, :] = 1.0\n",
    "\n",
    "# Setup data generator with various augmentations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Reshape for Keras (needs a batch dimension)\n",
    "x = sample_image.reshape((1,) + sample_image.shape)\n",
    "\n",
    "# Visualize augmentations\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(4, 4, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "\n",
    "i = 1\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(batch[0])\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Augmentation {i}\")\n",
    "    i += 1\n",
    "    if i >= 16:\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display data augmentation table\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "augmentation_table = \"\"\"\n",
    "### Common Data Augmentation Techniques\n",
    "\n",
    "#### For Images:\n",
    "| Technique | Description | Implementation | When to Use |\n",
    "|-----------|-------------|----------------|------------|\n",
    "| **Rotation** | Rotate image by random angle | `rotation_range` in ImageDataGenerator | Most image tasks |\n",
    "| **Flipping** | Mirror image horizontally or vertically | `horizontal_flip`, `vertical_flip` | When orientation doesn't change class |\n",
    "| **Scaling** | Zoom in or out randomly | `zoom_range` | Most image tasks |\n",
    "| **Translation** | Shift image horizontally or vertically | `width_shift_range`, `height_shift_range` | Object recognition |\n",
    "| **Color Jittering** | Alter brightness, contrast, saturation | `brightness_range` | When lighting varies |\n",
    "| **Cutout/Random Erasing** | Blank out random rectangles | Custom implementation | Object detection, classification |\n",
    "\n",
    "#### For Text:\n",
    "| Technique | Description | When to Use |\n",
    "|-----------|-------------|------------|\n",
    "| **Synonym Replacement** | Replace words with synonyms | Sentiment analysis, classification |\n",
    "| **Random Insertion/Deletion** | Insert or delete random words | Short text classification |\n",
    "| **Backtranslation** | Translate to another language and back | When meaning preservation is important |\n",
    "| **Word Swapping** | Randomly swap adjacent words | Most NLP tasks |\n",
    "\n",
    "#### For Time Series:\n",
    "| Technique | Description | When to Use |\n",
    "|-----------|-------------|------------|\n",
    "| **Time Warping** | Stretch or compress segments | Most time series tasks |\n",
    "| **Magnitude Warping** | Adjust the magnitude | Sensor data, signals |\n",
    "| **Frequency Warping** | Modify frequency components | Audio, vibration data |\n",
    "| **Jittering** | Add random noise | Robust models for noisy data |\n",
    "\n",
    "### Data Augmentation Best Practices:\n",
    "1. **Choose domain-appropriate transformations** - Augmentations should preserve class information\n",
    "2. **Apply multiple transformations** - Combine techniques for greater variety\n",
    "3. **Set reasonable ranges** - Too extreme transformations may hurt training\n",
    "4. **Test impact** - Measure performance with and without augmentation\n",
    "5. **Consider computational cost** - On-the-fly augmentation can slow training\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(augmentation_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transfer-learning'></a>\n",
    "## 17. Transfer Learning\n",
    "\n",
    "### Key Learning Points:\n",
    "- Transfer learning uses knowledge gained from one task to improve performance on another\n",
    "- Pre-trained models act as feature extractors or starting points for fine-tuning\n",
    "- Drastically reduces training time and data requirements\n",
    "- Particularly powerful for image, audio, and language tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transfer Learning with pre-trained models\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Diagram to show transfer learning concept\n",
    "def plot_transfer_learning_diagram():\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw the pre-trained model box\n",
    "    pretrained = plt.Rectangle((0.1, 0.4), 0.35, 0.4, fill=True, color='lightblue', alpha=0.5)\n",
    "    ax.add_patch(pretrained)\n",
    "    ax.text(0.275, 0.65, 'Pre-trained Model\\n(e.g., ResNet, VGG)', \n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.275, 0.5, 'Trained on ImageNet\\n(1M+ images, 1000 classes)', \n",
    "            ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Draw the custom model box\n",
    "    custom = plt.Rectangle((0.55, 0.4), 0.35, 0.4, fill=True, color='lightgreen', alpha=0.5)\n",
    "    ax.add_patch(custom)\n",
    "    ax.text(0.725, 0.65, 'Custom Model\\nfor Your Task', \n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.725, 0.5, 'Small dataset\\n(e.g., 100s of images)', \n",
    "            ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Add arrow connecting them\n",
    "    ax.annotate('', xy=(0.55, 0.6), xytext=(0.45, 0.6),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    \n",
    "    # Add text explaining approaches\n",
    "    ax.text(0.5, 0.85, 'Transfer Learning Approaches', \n",
    "            ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Feature extraction approach\n",
    "    ax.text(0.5, 0.3, 'Feature Extraction:', \n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.5, 0.25, 'Freeze pre-trained layers, replace and retrain only the classifier', \n",
    "            ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Fine-tuning approach\n",
    "    ax.text(0.5, 0.15, 'Fine-Tuning:', \n",
    "            ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.5, 0.1, 'Initialize with pre-trained weights, then retrain some or all layers with a small learning rate', \n",
    "            ha='center', va='center', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Show the concept diagram\n",
    "plot_transfer_learning_diagram()\n",
    "\n",
    "# Compare popular pre-trained models\n",
    "models_info = {\n",
    "    \"ResNet50\": {\n",
    "        \"year\": \"2015\",\n",
    "        \"parameters\": \"25M\",\n",
    "        \"ImageNet Accuracy\": \"76.0%\",\n",
    "        \"Special Features\": \"Residual connections to solve vanishing gradient\",\n",
    "        \"Usage\": \"Balanced accuracy/size tradeoff\"\n",
    "    },\n",
    "    \"VGG16\": {\n",
    "        \"year\": \"2014\",\n",
    "        \"parameters\": \"138M\",\n",
    "        \"ImageNet Accuracy\": \"71.3%\",\n",
    "        \"Special Features\": \"Simple, uniform architecture\",\n",
    "        \"Usage\": \"Feature extraction, simple to understand\"\n",
    "    },\n",
    "    \"MobileNetV2\": {\n",
    "        \"year\": \"2018\",\n",
    "        \"parameters\": \"3.5M\",\n",
    "        \"ImageNet Accuracy\": \"71.8%\",\n",
    "        \"Special Features\": \"Inverted residuals and linear bottlenecks\",\n",
    "        \"Usage\": \"Mobile and edge applications\"\n",
    "    },\n",
    "    \"EfficientNetB0\": {\n",
    "        \"year\": \"2019\",\n",
    "        \"parameters\": \"5.3M\",\n",
    "        \"ImageNet Accuracy\": \"77.1%\",\n",
    "        \"Special Features\": \"Compound scaling method\",\n",
    "        \"Usage\": \"Efficient use of parameters\"\n",
    "    },\n",
    "    \"BERT-base\": {\n",
    "        \"year\": \"2018\",\n",
    "        \"parameters\": \"110M\",\n",
    "        \"Task\": \"NLP\",\n",
    "        \"Special Features\": \"Bidirectional training of Transformer\",\n",
    "        \"Usage\": \"Text classification, QA, sentiment analysis\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "import pandas as pd\n",
    "models_df = pd.DataFrame(models_info)\n",
    "display(models_df.transpose())\n",
    "\n",
    "# Example code for implementing transfer learning\n",
    "print(\"\\nExample code for transfer learning with ResNet50:\")\n",
    "code_example = \"\"\"\n",
    "# Load pre-trained model (without the top classifier)\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet\n",
    "    include_top=False,   # Don't include the ImageNet classifier at the top\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "# Use the pre-trained model's preprocessing\n",
    "x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
    "# The base model contains multiple layers\n",
    "x = base_model(x, training=False)\n",
    "# Convert features to a single 1280-element vector per image\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "# Add a dropout layer for regularization\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# A Dense classifier with number of classes equal to your task\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train (feature extraction)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Fine tuning (optional)\n",
    "# Unfreeze some layers for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-10]:  # Freeze all except last 10 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\"\"\"\n",
    "print(code_example)\n",
    "\n",
    "# Display transfer learning best practices\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "transfer_learning_tips = \"\"\"\n",
    "### Transfer Learning Best Practices\n",
    "\n",
    "#### When to Use Transfer Learning:\n",
    "- When you have a **small dataset** (hundreds to thousands of examples)\n",
    "- When your task is **similar** to the pre-training task\n",
    "- When you need to **reduce training time**\n",
    "- When you want to achieve **better performance** with limited data\n",
    "\n",
    "#### Choosing a Pre-trained Model:\n",
    "1. **Consider the source domain** - If available, choose a model pre-trained on data similar to yours\n",
    "2. **Model size vs. performance tradeoff** - Larger models may perform better but require more resources\n",
    "3. **Computational constraints** - Some models may be too large for your hardware\n",
    "4. **Framework compatibility** - Ensure the model is available in your framework of choice\n",
    "\n",
    "#### Implementation Strategies:\n",
    "\n",
    "**1. Feature Extraction** (easier, faster):\n",
    "- Freeze all pre-trained layers\n",
    "- Replace and train only the classifier head\n",
    "- Best when new dataset is small and similar to original dataset\n",
    "\n",
    "**2. Fine-Tuning** (better results, more complex):\n",
    "- Start with a pre-trained model\n",
    "- Unfreeze some or all layers\n",
    "- Continue training with a very small learning rate\n",
    "- Best when new dataset is large enough and somewhat different from original\n",
    "\n",
    "**3. Progressive Fine-Tuning**:\n",
    "- Start by training only the top layers\n",
    "- Gradually unfreeze deeper layers\n",
    "- Use lower learning rates for earlier layers\n",
    "\n",
    "#### Domain-Specific Considerations:\n",
    "\n",
    "| Domain | Popular Pre-trained Models | Notes |\n",
    "|--------|----------------------------|-------|\n",
    "| **Computer Vision** | ResNet, VGG, EfficientNet | Most models pre-trained on ImageNet |\n",
    "| **Natural Language** | BERT, GPT, RoBERTa | Pre-trained on large text corpora |\n",
    "| **Audio** | Wav2Vec, Whisper | Speech recognition, audio classification |\n",
    "| **Multi-modal** | CLIP, DALL-E | Connecting vision and language |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(transfer_learning_tips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive notebook summarizes the key learning points from the AI with Python course, covering everything from basic data analysis to advanced machine learning techniques. Here's what we've covered:\n",
    "\n",
    "### Data Fundamentals\n",
    "- **Data Analysis** - Understanding your dataset through visualization and statistics\n",
    "- **Data Normalization** - Transforming features for optimal algorithm performance\n",
    "- **Handling Missing Values and Categorical Data** - Essential preprocessing techniques\n",
    "\n",
    "### Machine Learning Algorithms\n",
    "- **Linear & Logistic Regression** - The foundations of predictive modeling\n",
    "- **Decision Trees** - Interpretable models for classification and regression\n",
    "- **Support Vector Machines** - Powerful classifiers for complex datasets\n",
    "- **K-Means Clustering** - Unsupervised learning for pattern discovery\n",
    "\n",
    "### Neural Networks\n",
    "- **Neural Network Fundamentals** - Understanding neurons, weights, and backpropagation\n",
    "- **Activation Functions** - Adding non-linearity to model complex patterns\n",
    "- **Loss Functions** - Measuring and optimizing model performance\n",
    "- **Network Layers** - Building blocks for designing custom architectures\n",
    "- **Regression & Classification** - Applications to various problem types\n",
    "\n",
    "### Advanced Techniques\n",
    "- **Hyperparameter Tuning** - Optimizing model configuration\n",
    "- **Data Augmentation** - Expanding training datasets artificially\n",
    "- **Transfer Learning** - Leveraging pre-trained models for new tasks\n",
    "- **Model Optimization** - Strategies for faster and more efficient models\n",
    "\n",
    "By mastering these techniques, you can apply AI to a wide range of problems across different domains - from image recognition to natural language processing to time series forecasting. The field continues to evolve rapidly, but these fundamental concepts provide a solid foundation for further exploration and practical applications.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Explore Reinforcement Learning** - Agents that learn through interaction with environments\n",
    "- **Dive Deeper into Transformers** - The architecture behind modern NLP models\n",
    "- **Study Generative Models** - GANs, VAEs, and diffusion models for content creation\n",
    "- **Practice with Real-World Projects** - Apply these techniques to solve meaningful problems\n",
    "- **Keep Learning** - The field of AI evolves rapidly, stay curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: K-means on the California Housing dataset\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "housing_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "housing_clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize clusters in 2D PCA space\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(housing_pca[:, 0], housing_pca[:, 1], c=housing_clusters, cmap='viridis', alpha=0.7)\n",
    "plt.title('Housing Data Clustered with K-means (PCA Visualization)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook summarizes the key learning points from the AI with Python course. Here's what we've covered:\n",
    "\n",
    "1. **Data Analysis** - Exploration, visualization, handling missing values, and encoding categorical data\n",
    "2. **Linear Regression** - Predicting continuous values with linear models\n",
    "3. **Logistic Regression** - Binary classification with probability models\n",
    "4. **Decision Trees** - Interpretable models for classification and regression\n",
    "5. **Neural Networks for Regression** - Modeling complex relationships for continuous targets\n",
    "6. **Neural Networks for Classification** - Including CNNs for image data\n",
    "7. **Support Vector Machines** - Finding optimal decision boundaries\n",
    "8. **K-Means Clustering** - Unsupervised learning for finding patterns\n",
    "\n",
    "By mastering these techniques, you can apply AI to a wide range of problems across different domains.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore reinforcement learning\n",
    "- Dive deeper into natural language processing\n",
    "- Learn more about advanced neural network architectures\n",
    "- Apply these techniques to real-world projects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
